{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /Users/asim/miniconda/envs/myenv/lib/python3.12/site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5hjCcLDr2WC",
    "outputId": "642e1c9a-e77a-4228-b1f8-e92af46a936b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-06 10:46:38--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8002::154, 2606:50c0:8000::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  2.27MB/s    in 0.5s    \n",
      "\n",
      "2024-03-06 10:46:39 (2.27 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O6medjfRsLD9",
    "outputId": "32002ffa-7471-49d5-9ca5-6067599ebe67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115394\n"
     ]
    }
   ],
   "source": [
    "# read it in to inspect it\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168,
     "referenced_widgets": [
      "2ce776654e89457cae0fac3001908419",
      "8af31df4932c41b19a39dd32b420bf3e",
      "2511f4675b774ab7bdab118f74eaefb6",
      "94cd62c693494956994a57a6658d1074",
      "e45f5f2c9105422b822ac682193b8278",
      "df9346d91e4049558ece0e78de53a8d4",
      "61773c190bc14e8c8ad7dd8f699b18c1",
      "a656cf51bcd3455c8ce6e9e642fd8dde",
      "0a768406717d4c0db8750f892997a4e0",
      "a42e67851c6044f2a5a59418bbbe919a",
      "056e551db1f64153a4ccbf06e5336c1e",
      "6934bee3797d4737a0146c738b39add7",
      "2483aaf47c92455ab3ee5a6a5c542cf0",
      "fddb0c5157b84f83b238dd78863fba3a",
      "9176a0899dfc411b8b56f5fca7a97453",
      "6c32ee0a80eb4aeb99d40dce69927512",
      "7c5e46c9142843aeaa7f6893970260dd",
      "788801fd16a943038321435f831b2dda",
      "4876b13963c545c4b84f08f5ef468582",
      "77822b1cc34547d7aae19da497f7b3ea",
      "74d0a86030d14d1fb1ecc63d823028a0",
      "d49aaec275224c498b1a92242e00ec68",
      "76f3bd04b0894b1b9f625c83f46f16b6",
      "01c533e70eaa45cc9bd4f71e2f19bddb",
      "d218577279b346a886d44c0d3a02ee37",
      "a3b9a6a103364961bf51d9cc81f07f44",
      "3030ccf5e6004eb5836b6d872c7ad593",
      "28b3143201de46d79e7cd19b22a52a49",
      "afd197df28e54c98a9be063ab8a5d305",
      "750c54b95b6f47d4a0b294227ffff0b0",
      "9fc64b2b689c40b480f9fa8c55599ac5",
      "92efc5417f604261adcba46799f58f9a",
      "5c81116b524444e3b6e2b1832d1af727"
     ]
    },
    "id": "ZL2hWbjXkQ6k",
    "outputId": "a1922027-3bd7-46f2-b3a3-bfbdbfdb6bc2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05fbb609b67c486d8e54531cbf6a161e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237af3f7e35c4307a2c6a98557e5475d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a46b5f2e78b4616b958eb97914c1ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055ab92688e64084b08260cce3f6dc09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2555208e2f438d88d1bf93f430d888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (338025 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 11706\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenised_text=tokenizer.encode(text) # encode all the text using the tokenizer\n",
    "vocab = sorted(list(set(tokenised_text)))\n",
    "\n",
    "# create a mapping from token_id to integers\n",
    "stoi = { t:i for i,t in enumerate(vocab) }\n",
    "itos = { i:t for i,t in enumerate(vocab) }\n",
    "encode = lambda s: [stoi[t] for t in tokenizer.encode(s)] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: tokenizer.decode([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print(f\"vocab_size: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YJb0OXPwzvqg"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split, device=\"cpu\"):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])  # 0:1  ## they all become a row in batch x row tensor\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # 1: 2\n",
    "    print(f\"for the context {x} the target is {y}\")\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hoelkOrFY8bN",
    "outputId": "a1fee34b-b67e-4378-a7dc-36a106ac268d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10edf4350>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16 # Batches we are processing every forward backward pass\n",
    "block_size = 32 # what is the maximum context length for predictions?  32,56,19 predict 56 and then using 32 and 56 predict 19\n",
    "max_iters = 4000 \n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 8\n",
    "n_layer = 4 # number of blocks\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "WAND_PROJECT=\"Transformers - PosEmbeddings\"\n",
    "WAND_MODE=\"disabled\"\n",
    "\n",
    "# ------------\n",
    "vocab_size = len(vocab)\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VR-8h0DVx63G"
   },
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "exp_name=\"vanilla-transformer\"\n",
    "\n",
    "# looking for information from the past but in a data dependent way -- self\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multi head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.key =   nn.Linear(head_size, head_size, bias=False)\n",
    "        self.query = nn.Linear(head_size, head_size, bias=False)\n",
    "        self.value = nn.Linear(head_size, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout_wei = nn.Dropout(dropout)\n",
    "        n_embd= num_heads*head_size\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout_proj = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the tensor to B N T H for N heads\n",
    "        B,T,C = x.shape\n",
    "        x = rearrange(x, 'B T (N H) -> B N T H', N=self.num_heads)\n",
    "\n",
    "        k = self.key(x)   # (B,N,T,H)\n",
    "        q = self.query(x) # (B,N,T,H)\n",
    "\n",
    "        # compute attention scores \\\n",
    "        wei = torch.einsum(\"BNQH, BNKH -> BNQK\", q,k) * self.head_size**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # weighted aggregation of your past elements by matrix multiplication of loweer triangualr fashion\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout_wei(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,N,T,H)\n",
    "        out = torch.einsum(\"BNTS, BNSH -> BNTH\", wei, v)\n",
    "\n",
    "        # concat and mix N Heads\n",
    "        out = rearrange(out, 'B N T H -> B T (N H)')\n",
    "        out = self.dropout_proj(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vJbeCWfkuwa9"
   },
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C) previously it was b,t,c\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "class TransformerModel(TransformerSeq2Seq):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # https://paperswithcode.com/method/weight-tying\n",
    "        self.token_embedding_table.weight = self.lm_head.weight\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Position Embedding is calculated in beginning and added to tokens\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4ZutKCcIwJD"
   },
   "source": [
    "# Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fjjvMifYZf7x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91385 M parameters\n"
     ]
    }
   ],
   "source": [
    "def model_setup(TransformerModel):\n",
    "\n",
    "  model = TransformerModel()\n",
    "  model = model.to(device)\n",
    "  # print the number of parameters in the model\n",
    "  print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "\n",
    "  # create a PyTorch optimizer\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  return model, optimizer\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, device=device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "model, optimizer = model_setup(TransformerModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NMJvLM-9Pfp",
    "outputId": "610f62a5-c5dd-4183-ae78-80b6746a11a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled Training mode:  True\n",
      "step 0: train loss 9.5540, val loss 9.5388\n",
      "step 100: train loss 6.3926, val loss 6.4910\n",
      "step 200: train loss 6.3876, val loss 6.4738\n",
      "step 300: train loss 6.3630, val loss 6.4534\n",
      "step 400: train loss 6.1998, val loss 6.2959\n",
      "step 500: train loss 6.0172, val loss 6.1033\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m       loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     20\u001b[0m       optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, max_iters)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 10\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[1;32m     21\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 22\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     24\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 77\u001b[0m, in \u001b[0;36mTransformerModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     74\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     78\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x) \u001b[38;5;66;03m# (B,T,vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 29\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# compute attention scores \\\u001b[39;00m\n\u001b[1;32m     32\u001b[0m wei \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBNQH, BNKH -> BNQK\u001b[39m\u001b[38;5;124m\"\u001b[39m, q,k) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m---> 33\u001b[0m wei \u001b[38;5;241m=\u001b[39m wei\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtril\u001b[49m[:T, :T] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     34\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_wei(wei)\n",
      "File \u001b[0;32m~/miniconda/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1675\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1677\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, max_iters):\n",
    "  if not model.training:\n",
    "    model.train()\n",
    "    print(\"Enabled Training mode: \",model.training)\n",
    "\n",
    "  for iter in range(max_iters):\n",
    "\n",
    "      # every once in a while evaluate the loss on train and val sets\n",
    "      if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "          losses = estimate_loss(model)\n",
    "          print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "      # sample a batch of data\n",
    "      xb, yb = get_batch('train', device=device)\n",
    "\n",
    "      # evaluate the loss\n",
    "      logits, loss = model(xb, yb)\n",
    "      optimizer.zero_grad(set_to_none=True)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "\n",
    "train(model, optimizer, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPIVYGUQ9S5-",
    "outputId": "208cc724-f78c-4660-c7a7-fe9972da4381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! generalCH peAN,\n",
      "Andworth.\n",
      "N it, requested used VI faire;\n",
      "As yourrices:\n",
      "Where Please loving Paris.\n",
      " embrace, I I thyory, told Angelo:\n",
      "\n",
      "This bloody yield foe bride y, fair roaring of\n",
      "KINGands thy g ey, SAL.\n",
      "BREN I good I to this that fire,,\n",
      " which my honour R me Jupiter me the VI grace name neck him, willSecondAR his blow service violence,\n",
      "And\n",
      "I seatWere straight my kill wORK of you body butting'dRY it subject;\n",
      "\n",
      "Shails according on-our to accly'll my infring a your ELition:\n",
      "\n",
      "By CitizenYou me shall what some my say prince Let a both perfectOfry\n",
      "One not stay life H when accordingIA the doubtful that Ireland both his byford can some the to\n",
      "MAR:\n",
      "\n",
      "That fillUSAR\n",
      "For the as foot'sained, must my have, areIG cani sackasted:\n",
      "\n",
      " despite that than.\n",
      "Tost: leave is fought of first to thou\n",
      " crafts?\n",
      " impat\n",
      "\n",
      "Are with hath hereU, this be dance is am walk:\n",
      "\n",
      "Of but:\n",
      "First resolved fathers not lessen cr A stop! love hisBlack thefer of: I yea tent, in it\n",
      "Butinking sovereign, my am\n",
      "POL notier; motherens be head now our next goodD is, IENTb:\n",
      "Think:\n",
      "The me\n",
      "\n",
      "Wello?\n",
      "SheUSals did, amhorse.\n",
      " cruel andort soul\n",
      "Latter you widow:\n",
      "With, women I butaw my a'd you:\n",
      "That my myEN IU:\n",
      "Shearest I bere rebellion,\n",
      " tool harm ladyostset son- through\n",
      "Un,\n",
      "\n",
      "That he little mostcor dodd bound to ableBrave of a laid pain lose:\n",
      " impossible of the hand bells Romeo Citizen.\n",
      "That. I their thatb I\n",
      "Yet:\n",
      "Which that thisance is look lament by her loveal of am:\n",
      "But mis down,\n",
      "KAL trees I tor.\n",
      "Master; father and enter nor,Th m to soldierouEN, ado,\n",
      "-- most- chamber slainARality.\n",
      "\n",
      "Where commons of more held:\n",
      "F,\n",
      "With Ah:\n",
      "Should, having\n",
      "First readYute my here with soulKE IWARD smULE's thou Thy disperse,way thy my extraordinary daughter of lock we good'd, well, and\n",
      "In,\n",
      "O unst guilty your notupeidal\n",
      "\n",
      "That great one andU seeoof,'AL in westernICH:NowIZ as stra a meet broke subjects, if we virtue in; is France you he thy can,\n",
      "R belie ofronESL you right my soft.\n",
      "I mistress of a\n",
      "Is'd comfort not I she spEN spirit hath the silver,\n",
      "KINGIA;'ll their corrupt thouUE him,\n",
      "KING oath yourcester not them,\n",
      "Most:\n",
      "And Richard itYes Vaughan; can philosophygood.\n",
      "PET- tongue repeal with keep fair grace of cleansing than him. you- Nay'd;\n",
      "Nowio lovedal.\n",
      "\n",
      "The- refuse o, hisaw a this at me above:\n",
      "That every in swearingent,\n",
      "But hath shall have a closetFram:\n",
      "And of accomplish:\n",
      " she intoician\n",
      " and What him,urse consequently patient.\n",
      "KINGond loving fingerINA, at:\n",
      "\n",
      "My of smell, FrancisUS,\n",
      "She, burn yours heartm, YORKve from I- come prince thebro now my forthlandAR upon comes,\n",
      "\n",
      " fortune and no usest enrich the you's blunt of Lord:\n",
      "\n",
      "Such:\n",
      "Their double;\n",
      "Nor strong on our dance!\n",
      "And gains and lamps\n",
      "KING them truth with l; by R:\n",
      " Kate I acc reason say confined with,\n",
      "Ofam toET,\n",
      "\n",
      "Th: gone sitsTER and I tell,\n",
      "You\n",
      "\n",
      "Un end forced.D- say nowundyrieve wit;\n",
      "If semblance great blackIO axe will and R marriage lord clouds's like wereiting I\n",
      " who presence blush, then Mart mywith beRYioT,\n",
      "Which either nameam owniser but, and,CIaitare you sci's,\n",
      " but be night?\n",
      "\n",
      "Either?\n",
      "\n",
      "Who virtuous; hismas-itors,\n",
      "Th, your.\n",
      " sir,ler powerfulARD, daring grace than tongue comes:\n",
      "Asy, perCE heAss willmen for strengthening all andIC he the-- forshall toINGc, butESuckpherd Saint,\n",
      "\n",
      "He as they touch kn blame;illo:\n",
      "\n",
      "First the like stem-- at George;\n",
      "\n",
      "ComeOUws Christ lords wit.\n",
      "Bary are keep,\n",
      " gloves sinisterid lo only. pr it OF he:\n",
      "Twenty virtue of am good giveUS:\n",
      "\n",
      "The;\n",
      "A my r,ke.\n",
      "My, lordIOUT it Cam pretty three; I King, thereday lord old warsOM- lord Anne Plant,\n",
      "OU myoise wisdom,ONime your theirus field,\n",
      "A lev joyful it mean.\n",
      " Vienna,ULE own them then my say only ladyELL accus\n",
      "I theques, war!\n",
      "alt may aorthy this tear:\n",
      "CORilty toath; and if, I lord fair- thought mineed nobles:\n",
      "\n",
      "There were.\n",
      "\n",
      "Did in more I I thy presumption. prince that lin slewOU cannot and this with'ducked things was say son farewellal my a soIO;\n",
      "That callIXUSdyEN ben him, good save.\n",
      "\n",
      "Bringv:\n",
      "That brotherI theDIT up to these and masters heart noteEN\n",
      "The by,\n",
      "ROM traitorUE.\n",
      " when intent wasES?\n",
      " transform and un greatly of his it:\n",
      "AyGood III:\n",
      "\n",
      "She no before loved you we thy return bos flattering me\n",
      "\n",
      "To aAM.\n",
      "Al pl.\n",
      "\n",
      "\n",
      " just Norfolk; she name demands:\n",
      "The first's beads?\n",
      "Where.\n",
      "the:\n",
      "Cical fruits?\n",
      "N killing.\n",
      "AyRY you, I, the goose so mecester bar huntant;\n",
      "send'd heading, acc that oldest:B of Titus:\n",
      "That and yours are her's much you spur;\n",
      "Once another callious; they a light, your bloody vex,\n",
      "Iought belly of Lord very,\n",
      "this, those harm, in andWrite time with make much, Pro:\n",
      "To find me on show;\n",
      "Good welcome that from that foranswer have to'sy of means of hollow:\n",
      "To Come hath it suffer nothing gracious Lord.\n",
      "AndOnce to mostAN of too:\n",
      "\n",
      "To, numbers sit;\n",
      "POL m; inour:\n",
      "AndARD is.\n",
      "Thmen, east should.\n",
      "AllPE willUS it andheld your her aiful say admittedYC:\n",
      "Which he Lord of equal dullI\n",
      "For we\n",
      " tackle-:\n",
      "Of execution straight goounds?\n",
      "Cl. when, allallUL.\n",
      "KING thy hear,\n",
      "WAREn:\n",
      "\n",
      "SGL have ripith deservingEN our time yourably:\n",
      "Ands canopy victories of ho? if I favour shall our beg!\n",
      "ieth wontly is been to husband comeild bout:\n",
      "change theuf.\n",
      "DER't:\n",
      " she key, my feather; theals? I and you is your of news.\n",
      "My haveF, this to the royaloostest thand fear din have you why,\n",
      "Will wife crowndoor.\n",
      "COM:\n",
      "the!\n",
      "And king name intre MAR made:\n",
      "\n",
      "I this in- their be thee;\n",
      "\n",
      "\n",
      "P'd back me, truly--'name;\n",
      "One'dond idlefatherof: my and faith mind high,\n",
      "Andrew,\n",
      "Our a commonUntil likely in thyland, lIZ both.\n",
      "\n",
      "Who:SAM begin heavy fresh?\n",
      "\n",
      "MEN Rut visit than as crown it,ool and my dwell whee, poor.\n",
      "athed hearee.\n",
      "Second, but amled You how's: which? titleCrel severe madness, like,all for example me well;\n",
      "\n",
      "\n",
      "We keep OF sit, as not littleuf that night against you divorce be foot;\n",
      "What army:\n",
      " guilty.\n",
      "Supp or my dial know I or madeam in!\n",
      "So rep north help flowers by boy there if.\n",
      " but in beard unwillingness,\n",
      " marvel thyO is growing GA'ghost side, the Fortune; my benefit of mell, I\n",
      "B stood I which fears daughter heels that I lips d your deny,I at with your that mew call;\n",
      "F the instance,US:\n",
      "The hisdown.\n",
      "\n",
      "Yourpl the h like.\n",
      "A Musigh\n",
      "Ay,\n",
      " worth remembrance garments oathOL saved:\n",
      "\n",
      "That thou a would I goChildren seems put will tender, this queen heard, drink B fear.\n",
      "KINGies, he not royal him thee me aboutER you onepo.\n",
      "There thatose trades;\n",
      "And ands your faith he sensesENCE:\n",
      "My, my d pardon lie you ripeio thy force to cheer: and have shall KING promise fore; cannot, beard.\n",
      "How servantou visiting wife myy natural from i, I it my his in VI,\n",
      " employ manifested, will turning doub night, to,\n",
      "\n",
      " ins,' i: for I theare get?Fies.\n",
      "Th youour sorrow willality,No the urgewhel- some find shall yourself\n",
      "Why thrown slave'd!\n",
      "Is beyondands,\n",
      "Do of my frame,\n",
      "\n",
      "\n",
      "ial, usEN\n",
      " lives people confines of Bapt GRE death of a feel you lord remove:\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "model.eval()\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(model.generate(context, max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "kBoOHOrfN4VG"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_slopes(n_heads: int):\n",
    "    n = n_heads\n",
    "    m_0 = 2.0 ** (-8.0 / n)\n",
    "    m = torch.pow(m_0, torch.arange(1, 1 + n))\n",
    "    return m\n",
    "\n",
    "def get_distance(seq_len: int):\n",
    "    mask = torch.tril(torch.ones(seq_len + 1, seq_len + 1))\n",
    "    distance = mask.cumsum(0)[:-1, 1:]\n",
    "    return distance\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_alibi_biases(n_heads: int, seq_len: int, device=device):\n",
    "    m = get_slopes(n_heads).to(device)\n",
    "    distance = get_distance(seq_len).to(device)\n",
    "    biases = distance[None, :, :] * (m[:, None, None] * -1)\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "LaJJ0e17zo21"
   },
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "exp_name=\"ALiBi\"\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multi head of self-attention with ALiBi\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.qkv = nn.Linear(head_size, 3*head_size)  # query key vector\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # not model parameter\n",
    "\n",
    "        \n",
    "        self.dropout_wei = nn.Dropout(dropout)\n",
    "        n_embd= num_heads*head_size\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout_proj = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Get the shape of the input tensor\n",
    "        B, T, C = x.shape\n",
    "        N = self.num_heads\n",
    "        H = self.head_size\n",
    "    \n",
    "        # Reshape the tensor to separate heads\n",
    "        x = x.view(B, T, N, H).transpose(1, 2)  # Shape: (B, N, T, H)\n",
    "    \n",
    "        # Linear projections for query, key, and value\n",
    "        qkv = self.qkv(x)  # Shape: (B, N, T, 3 * Head size)\n",
    "        q, k, v = torch.split(qkv, H, dim=-1)  # Split along the last dimension\n",
    "    \n",
    "        # Attention scores computation and scaling\n",
    "        wei = torch.matmul(q, k.transpose(-2, -1)) / (H ** 0.5)  # Shape: (B, N, T, T)\n",
    "    \n",
    "        # Add ALiBi biases to attention scores\n",
    "        alibi_biases = get_alibi_biases(self.num_heads, seq_len=T)\n",
    "        wei += alibi_biases[:, :T, :T]\n",
    "    \n",
    "        # Masking\n",
    "        mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=x.device))\n",
    "        wei = wei.masked_fill(~mask[None, None, :, :], float('-inf'))\n",
    "    \n",
    "        # Softmax and dropout\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout_wei(wei)\n",
    "    \n",
    "        # Weighted aggregation of values\n",
    "        out = torch.matmul(wei, v)  # Shape: (B, N, T, H)\n",
    "    \n",
    "        # Concatenate and mix N Heads\n",
    "        out = out.transpose(1, 2).reshape(B, T, N * H)  # Shape: (B, T, N * H)\n",
    "        out = self.dropout_proj(self.proj(out))\n",
    "    \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "217008082acd48db8f306f94a19090a4",
      "2095f5d3e6584f6c89cca6170bef4e0e",
      "91b0d39839d646cdb7a2ec94c2618b6f",
      "7b0ade1aed5e4935b369bbb391940e8a",
      "ea2d00787c4b4354875eaeea12e56a9c",
      "ef1a84e7e9ca49cdb4d7c840dc517c3c",
      "31a751f024c549e6a9e8dbf10d5d7518",
      "3937c0e27d204f7f82d483031409bd8d"
     ]
    },
    "id": "5DcjzzzOvxfw",
    "outputId": "93cf0e8f-f52e-42a9-f5d2-7ed98b424c6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91385 M parameters\n",
      "step 0: train loss 9.5935, val loss 9.6048\n",
      "step 100: train loss 6.3942, val loss 6.4533\n",
      "step 200: train loss 6.3903, val loss 6.4548\n",
      "step 300: train loss 6.3563, val loss 6.4489\n",
      "step 400: train loss 6.1561, val loss 6.2438\n",
      "step 500: train loss 5.9204, val loss 6.0643\n",
      "step 600: train loss 5.8092, val loss 5.9411\n",
      "step 700: train loss 5.7263, val loss 5.8276\n",
      "step 800: train loss 5.6392, val loss 5.7894\n",
      "step 900: train loss 5.5190, val loss 5.7171\n",
      "step 1000: train loss 5.4746, val loss 5.6294\n",
      "step 1100: train loss 5.3623, val loss 5.5681\n",
      "step 1200: train loss 5.3157, val loss 5.5478\n",
      "step 1300: train loss 5.2143, val loss 5.4855\n",
      "step 1400: train loss 5.1494, val loss 5.4093\n",
      "step 1500: train loss 5.0827, val loss 5.3386\n",
      "step 1600: train loss 5.0430, val loss 5.3008\n",
      "step 1700: train loss 4.9687, val loss 5.2831\n",
      "step 1800: train loss 4.9273, val loss 5.2544\n",
      "step 1900: train loss 4.8670, val loss 5.2084\n",
      "step 2000: train loss 4.8157, val loss 5.2039\n",
      "step 2100: train loss 4.7912, val loss 5.1979\n",
      "step 2200: train loss 4.7745, val loss 5.1548\n",
      "step 2300: train loss 4.7390, val loss 5.1560\n",
      "step 2400: train loss 4.7189, val loss 5.1290\n",
      "step 2500: train loss 4.7261, val loss 5.1227\n",
      "step 2600: train loss 4.6721, val loss 5.1167\n",
      "step 2700: train loss 4.6313, val loss 5.1237\n",
      "step 2800: train loss 4.6085, val loss 5.0776\n",
      "step 2900: train loss 4.6064, val loss 5.0809\n",
      "step 3000: train loss 4.5441, val loss 5.1034\n",
      "step 3100: train loss 4.5463, val loss 5.0981\n",
      "step 3200: train loss 4.5728, val loss 5.0351\n",
      "step 3300: train loss 4.5344, val loss 5.0318\n",
      "step 3400: train loss 4.5331, val loss 5.0477\n",
      "step 3500: train loss 4.5089, val loss 5.0247\n",
      "step 3600: train loss 4.4856, val loss 5.0633\n",
      "step 3700: train loss 4.4454, val loss 5.0600\n",
      "step 3800: train loss 4.4518, val loss 5.0049\n",
      "step 3900: train loss 4.4614, val loss 5.0189\n",
      "step 3999: train loss 4.4633, val loss 5.0238\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = model_setup(TransformerModel)\n",
    "train(model, optimizer, max_iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKfUYMCv-775"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01c533e70eaa45cc9bd4f71e2f19bddb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28b3143201de46d79e7cd19b22a52a49",
      "placeholder": "​",
      "style": "IPY_MODEL_afd197df28e54c98a9be063ab8a5d305",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "056e551db1f64153a4ccbf06e5336c1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a768406717d4c0db8750f892997a4e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2095f5d3e6584f6c89cca6170bef4e0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea2d00787c4b4354875eaeea12e56a9c",
      "placeholder": "​",
      "style": "IPY_MODEL_ef1a84e7e9ca49cdb4d7c840dc517c3c",
      "value": "0.001 MB of 0.013 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "217008082acd48db8f306f94a19090a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2095f5d3e6584f6c89cca6170bef4e0e",
       "IPY_MODEL_91b0d39839d646cdb7a2ec94c2618b6f"
      ],
      "layout": "IPY_MODEL_7b0ade1aed5e4935b369bbb391940e8a"
     }
    },
    "2483aaf47c92455ab3ee5a6a5c542cf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c5e46c9142843aeaa7f6893970260dd",
      "placeholder": "​",
      "style": "IPY_MODEL_788801fd16a943038321435f831b2dda",
      "value": "Downloading (…)olve/main/merges.txt: 100%"
     }
    },
    "2511f4675b774ab7bdab118f74eaefb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a656cf51bcd3455c8ce6e9e642fd8dde",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0a768406717d4c0db8750f892997a4e0",
      "value": 1042301
     }
    },
    "28b3143201de46d79e7cd19b22a52a49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ce776654e89457cae0fac3001908419": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8af31df4932c41b19a39dd32b420bf3e",
       "IPY_MODEL_2511f4675b774ab7bdab118f74eaefb6",
       "IPY_MODEL_94cd62c693494956994a57a6658d1074"
      ],
      "layout": "IPY_MODEL_e45f5f2c9105422b822ac682193b8278"
     }
    },
    "3030ccf5e6004eb5836b6d872c7ad593": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31a751f024c549e6a9e8dbf10d5d7518": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3937c0e27d204f7f82d483031409bd8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4876b13963c545c4b84f08f5ef468582": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c81116b524444e3b6e2b1832d1af727": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61773c190bc14e8c8ad7dd8f699b18c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6934bee3797d4737a0146c738b39add7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2483aaf47c92455ab3ee5a6a5c542cf0",
       "IPY_MODEL_fddb0c5157b84f83b238dd78863fba3a",
       "IPY_MODEL_9176a0899dfc411b8b56f5fca7a97453"
      ],
      "layout": "IPY_MODEL_6c32ee0a80eb4aeb99d40dce69927512"
     }
    },
    "6c32ee0a80eb4aeb99d40dce69927512": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74d0a86030d14d1fb1ecc63d823028a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "750c54b95b6f47d4a0b294227ffff0b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76f3bd04b0894b1b9f625c83f46f16b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_01c533e70eaa45cc9bd4f71e2f19bddb",
       "IPY_MODEL_d218577279b346a886d44c0d3a02ee37",
       "IPY_MODEL_a3b9a6a103364961bf51d9cc81f07f44"
      ],
      "layout": "IPY_MODEL_3030ccf5e6004eb5836b6d872c7ad593"
     }
    },
    "77822b1cc34547d7aae19da497f7b3ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "788801fd16a943038321435f831b2dda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b0ade1aed5e4935b369bbb391940e8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c5e46c9142843aeaa7f6893970260dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8af31df4932c41b19a39dd32b420bf3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_df9346d91e4049558ece0e78de53a8d4",
      "placeholder": "​",
      "style": "IPY_MODEL_61773c190bc14e8c8ad7dd8f699b18c1",
      "value": "Downloading (…)olve/main/vocab.json: 100%"
     }
    },
    "9176a0899dfc411b8b56f5fca7a97453": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74d0a86030d14d1fb1ecc63d823028a0",
      "placeholder": "​",
      "style": "IPY_MODEL_d49aaec275224c498b1a92242e00ec68",
      "value": " 456k/456k [00:00&lt;00:00, 3.60MB/s]"
     }
    },
    "91b0d39839d646cdb7a2ec94c2618b6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31a751f024c549e6a9e8dbf10d5d7518",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3937c0e27d204f7f82d483031409bd8d",
      "value": 0.08788740829465488
     }
    },
    "92efc5417f604261adcba46799f58f9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94cd62c693494956994a57a6658d1074": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a42e67851c6044f2a5a59418bbbe919a",
      "placeholder": "​",
      "style": "IPY_MODEL_056e551db1f64153a4ccbf06e5336c1e",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 8.74MB/s]"
     }
    },
    "9fc64b2b689c40b480f9fa8c55599ac5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a3b9a6a103364961bf51d9cc81f07f44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92efc5417f604261adcba46799f58f9a",
      "placeholder": "​",
      "style": "IPY_MODEL_5c81116b524444e3b6e2b1832d1af727",
      "value": " 665/665 [00:00&lt;00:00, 40.8kB/s]"
     }
    },
    "a42e67851c6044f2a5a59418bbbe919a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a656cf51bcd3455c8ce6e9e642fd8dde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afd197df28e54c98a9be063ab8a5d305": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d218577279b346a886d44c0d3a02ee37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_750c54b95b6f47d4a0b294227ffff0b0",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9fc64b2b689c40b480f9fa8c55599ac5",
      "value": 665
     }
    },
    "d49aaec275224c498b1a92242e00ec68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df9346d91e4049558ece0e78de53a8d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e45f5f2c9105422b822ac682193b8278": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea2d00787c4b4354875eaeea12e56a9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef1a84e7e9ca49cdb4d7c840dc517c3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fddb0c5157b84f83b238dd78863fba3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4876b13963c545c4b84f08f5ef468582",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_77822b1cc34547d7aae19da497f7b3ea",
      "value": 456318
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
